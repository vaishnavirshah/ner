# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNA8OVCGKdFuzi4f0Zr61BAsxxlag01V
"""

pip install plac

from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/MyDrive/RA/'

path = '/content/drive/MyDrive/RA/'

from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from tqdm import tqdm
from wordcloud import WordCloud, STOPWORDS
from spacy.util import minibatch, compounding
from spacy.training.example import Example
import matplotlib
import re
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

nlp = spacy.load('en_core_web_sm')

data = pd.read_csv(path+'MiniProject/crawl_data.csv', error_bad_lines=False, engine='python')
data.head()

chemical_name = pd.read_csv(path+'MiniProject/drug_chem.csv', error_bad_lines=False, engine='python')
chemical_name.head()

chemical = chemical_name['chemical_name'].unique().tolist()
chemical = [x.lower() for x in chemical]
print(chemical)

street = street_name['street_names'].unique().tolist()
more_names = street_name['more_names'].unique().tolist()
street = [x.lower() for x in street]
street.append('street')

street_name = pd.read_csv(path+'MiniProject/drug_street.csv', error_bad_lines=False, engine='python')
street_name.head()

data = data.dropna()

data.info()

stopwords = ['i', 'me', 'skip','section','-','site', 'no', 'javascript', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

def apwords(words):
    filtered_sentence = []
    words = word_tokenize(words)
    for w in words:
        if w in stopwords:
            continue
        else:
            filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: apwords(x)

data['modified_body'] = data['body'].apply(addwords)
data['modified_title'] = data['title'].apply(addwords)
data.drop('body', axis=1, inplace=True)
data.drop('description', axis=1, inplace=True)

data.head()

data['modified_body']

# potential_street_name = chemical_name['potential_street_name'].unique().tolist()
# potential_street_name = [str(x) for x in potential_street_name if (str(x) != 'nan') and (str(x) != '0')]
# for i in potential_street_name:
#   street.append(i)

# more_names = [str(x) for x in more_names if (str(x) != 'nan') and (str(x) != '0')]
# more_names = [x.lower() for x in more_names]

# for i in more_names:
#   street.append(i)

print(street)

def process_info(token_set): # basically combining it
  processed_info = []
  for token in token_set:
    token = ''.join(e.lower() for e in token if e.isalnum())
    processed_info.append(token)
  return ' '.join(processed_info)

count = 0
Train_Data_Body = []
Train_Data_Desc = []
Train_Data_Title =[]

for _, item in data.iterrows():
    ent_dict = {}
    if count<10000:
        info = process_info(item['modified_title'])
        visited_items =[]
        entities = []

        for token in info.split():
            if token in street:
              #print(token)
              for i in re.finditer(token, info):
                  if token  not in visited_items:
                      entity = (i.span()[0], i.span()[1], 'STREET')
                      #print(entity)
                      visited_items.append(token)
                      entities.append(entity)
              if token in chemical:
              #print(token)
                for i in re.finditer(token, info):
                    if token  not in visited_items:
                        entity = (i.span()[0], i.span()[1], 'CHEMICAL')
                        print(entity)
                        visited_items.append(token)
                        entities.append(entity)
        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            #print(train_item)
            Train_Data_Title.append(train_item)
            count+=1

for _, item in data.iterrows():
    ent_dict = {}
    if count<1000:
        info = process_info(item['modified_body'])

        visited_items =[]
        entities = []

        for token in info.split():
            if token in street:
              #print(token)
              for i in re.finditer(token, info):
                  if token  not in visited_items:
                      entity = (i.span()[0], i.span()[1], 'STREET')
                      #print(entity)
                      visited_items.append(token)
                      entities.append(entity)
              if token in chemical:
              #print(token)
                for i in re.finditer(token, info):
                    if token  not in visited_items:
                        entity = (i.span()[0], i.span()[1], 'CHEMICAL')
                        print(entity)
                        visited_items.append(token)
                        entities.append(entity)
        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            #print(train_item)
            Train_Data_Body.append(train_item)
            count+=1

#Train_Data_Title

#Train_Data_Body

n_iter = 50
def train_ner_model(train_data):
    training_data = train_data
    nlp = spacy.blank("en")

    if "ner" not in nlp.pipe_names:
        nlp.add_pipe('ner')
        ner = nlp.get_pipe("ner")
    else:
        ner = nlp.get_pipe("ner")

    for _, annotations in training_data:
        for ent in annotations.get("entities"):
            ner.add_label(ent[2])

    nlp.begin_training()
    for itn in range(n_iter):
        random.shuffle(training_data)
        losses = {}

        batches = minibatch(training_data, size=compounding(4.0,32.0,1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            example = []
            for i in range(len(texts)):
                doc = nlp.make_doc(texts[i])
                example.append(Example.from_dict(doc, annotations[i]))
            nlp.update(example,drop=0.5,losses = losses)
        print("Losses: ",itn,' ', losses)
    return nlp

nlp_model = train_ner_model(Train_Data_Title)

example_statement = "I think Drugs like Cannabis, Ecstacy should be done with hash and pot at a party in Mumbai"

docx2 = nlp_model(example_statement)
for entity in docx2.ents:
    print(entity, entity.label_)

drug_pages = 0
page_traversed = 0

for text,_ in Train_Data_Title[:]:
    page_traversed +=1
    doc = nlp_model(text)
    result = [(ent,ent.label_) for ent in doc.ents]
    if len(result):
        check = 1
        drug_pages += 1
    else:
        check = 0

check = 0
drug_pages_found = 0
titles_traversed = 0

def extract_drug_entity(text):
    check = 0
    drug_pages_found = 0
    titles_traversed = 0
    while (titles_traversed<17711):
        doc_extract = nlp_model(text)
        titles_traversed +=1
        result = [(ent,ent.label_) for ent in doc.ents]
        if len(result):
            print(result)
            check = 1
            drug_pages_found +=1
        else:
            check = 0
            drug_pages_found = drug_pages_found
    return check

print(drug_pages, page_traversed)

counter = 0

while counter <= 15:
    for text,_ in Train_Data_Body[:]:
        page_traversed +=1
        doc = nlp_model(text)
        result = [(ent,ent.label_) for ent in doc.ents]
        if len(result):
            check = 1
            drug_pages += 1
        else:
            check = 0
    counter+=1

print("Total pages viewed:", page_traversed)
print("Pages related to drugs:", drug_pages)