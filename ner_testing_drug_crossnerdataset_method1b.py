# -*- coding: utf-8 -*-
"""NER_testing-drug_crossnerDataset_Method1B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rb-WRZF3O-YXCi7luCxSYvq8YlMVVrBd

imports
"""

pip install plac

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/RA/'

from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from tqdm import tqdm
from wordcloud import WordCloud, STOPWORDS
from spacy.util import minibatch, compounding
from spacy.training.example import Example
import matplotlib
import re
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

nlp = spacy.load('en_core_web_sm')

"""testing data - cross_ner"""

test_data = pd.read_csv(path+'DRUG_CROSSNER-main/ner_data/drugs/test_updated.txt', error_bad_lines=False, engine='python',sep='\s',header=None)
test_data.head()

len(test_data)

"""Data for training - crawl_data"""

data = pd.read_csv(path+'MiniProject/crawl_data.csv', error_bad_lines=False, engine='python')
data.head()

"""drug chem file"""

chemical_name = pd.read_csv(path+'MiniProject/drug_chem.csv', error_bad_lines=False, engine='python')
#chemical_name.head()
chemical = chemical_name['chemical_name'].unique().tolist()
chemical = [x.lower() for x in chemical]
print(chemical)

"""drug street file"""

street_name = pd.read_csv(path+'MiniProject/drug_street.csv', error_bad_lines=False, engine='python')
#street_name.head()
street = street_name['street_names'].unique().tolist()
more_names = street_name['more_names'].unique().tolist()
street = [x.lower() for x in street]

data = data.dropna()

data.info()

"""filtered stop words ref: miniproject"""

stopwords = ['i', 'me', 'skip','section','-','site', 'no', 'javascript', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

def filtering(words):
    filtered_sentence = []
    words = word_tokenize(words)
    for w in words:
        if w in stopwords:
            continue
        else:
            filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: filtering(x)

data['modified_body'] = data['body'].apply(addwords)
data['modified_title'] = data['title'].apply(addwords)
data.drop('body', axis=1, inplace=True)
data.drop('description', axis=1, inplace=True)

data.head()

"""potential street name from chemical file added to street list"""

potential_street_name = chemical_name['potential_street_name'].unique().tolist()
potential_street_name = [str(x) for x in potential_street_name if (str(x) != 'nan') and (str(x) != '0')]
for i in potential_street_name:
  street.append(i)

more_names = [str(x) for x in more_names if (str(x) != 'nan') and (str(x) != '0')]
more_names = [x.lower() for x in more_names]

for i in more_names:
  street.append(i)

print(street)

"""merged the words in list to string"""

def process_info(token_set): # basically combining it
  processed_info = []
  for token in token_set:
    token = ''.join(e.lower() for e in token if e.isalnum())
    processed_info.append(token)
  return ' '.join(processed_info)

"""preparing training title and body"""

count = 0
Train_Data_Title =[]

for _, item in data.iterrows():
    ent_dict = {}
    if count<10000:
        info = process_info(item['modified_title'])
        visited_items_street =[]
        visited_items_chemical =[]
        entities = []

        for token in info.split():
              if token in chemical:
                for i in re.finditer(token, info):
                    if token  not in visited_items_street and token not in visited_items_chemical:
                        entity = (i.span()[0], i.span()[1], 'CHEMICAL')
                        visited_items_chemical.append(token)
                        entities.append(entity)
              if token in street:
                for i in re.finditer(token, info):
                  if token  not in visited_items_street and token not in visited_items_chemical:
                      entity = (i.span()[0], i.span()[1], 'STREET')
                      visited_items_street.append(token)
                      entities.append(entity)
        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            Train_Data_Title.append(train_item)
            count+=1

count = 0
Train_Data_Body = []
all_data = []
for _, item in data.iterrows():
    ent_dict = {}
    if count<1000000:
        info = process_info(item['modified_body'])
        all_data.append(info)
        visited_items_street =[]
        visited_items_chemical =[]
        entities = []

        for token in info.split():
              if token in chemical:
                for i in re.finditer(token, info):
                    if token  not in visited_items_street and token not in visited_items_chemical:
                        entity = (i.span()[0], i.span()[1], 'CHEMICAL')
                        visited_items_chemical.append(token)
                        entities.append(entity)
              if token in street:
                for i in re.finditer(token, info):
                  if token  not in visited_items_street and token not in visited_items_chemical:
                      entity = (i.span()[0], i.span()[1], 'STREET')
                      visited_items_street.append(token)
                      entities.append(entity)
        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            Train_Data_Body.append(train_item)
            count+=1

"""training the model"""

n_iter = 10
def train_ner_model(train_data):
    training_data = train_data
    nlp = spacy.blank("en")

    if "ner" not in nlp.pipe_names:
        nlp.add_pipe('ner')
        ner = nlp.get_pipe("ner")
    else:
        ner = nlp.get_pipe("ner")

    for _, annotations in training_data:
        for ent in annotations.get("entities"):
            ner.add_label(ent[2])

    nlp.begin_training()
    for itn in range(n_iter):
        random.shuffle(training_data)
        losses = {}

        batches = minibatch(training_data, size=compounding(4.0,32.0,1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            example = []
            for i in range(len(texts)):
                doc = nlp.make_doc(texts[i])
                example.append(Example.from_dict(doc, annotations[i]))
            nlp.update(example,drop=0.5,losses = losses)
        print("Losses: ",itn,' ', losses)
    return nlp

nlp_model = train_ner_model(Train_Data_Title)

nlp_model.to_disk(path+'NER_testing-drug_crossnerDataset/')

sentences = []
labels = []
i =0
while i < (len(test_data)):
  sentence = []
  label = []

  while (test_data[0][i] != "THIS_IS_A_NEW_SENTENCE") and (i < (len(test_data))):
    sentence.append(test_data[0][i])
    label.append(test_data[1][i])
    i = i+1
  i = i+1
  sentences.append(sentence)
  labels.append(label)

print(sentences)
print(labels)

"""testing the data"""

tp =0
fp =0
tn =0
fn =0
for i in range(len(sentences)):
  sentence = ''
  for j in range(len(sentences[i])):
    sentence += sentences[i][j]
    sentence += ' '
  docx = nlp_model(sentence)
  identified_as_street = []
  for entity in docx.ents:
      num = entity.start
      identified_as_street.append(num)
  for k in range(len(labels[i])):
    if labels[i][k] == 'O' and k not in identified_as_street:
      tn+=1
    if labels[i][k] == 'O' and k in identified_as_street:
      fp +=1
    if labels[i][k] != 'O' and k not in identified_as_street:
      tn +=1
    if labels[i][k] != 'O' and k in identified_as_street:
      tp +=1
precision = tp/(tp+fp)
recall = tp/(tp+fn)
accuracy = (tp+tn)/(tp+tn+fp+fn)
print('precision = ', precision)
print('recall = ', recall)
print('accuracy = ', accuracy)
print('F1 Score: ', (precision*recall)/(precision+recall))

"""increasing size of testing data

"""

testing = pd.read_csv(path+'DRUG_CROSSNER-main/ner_data/drugs/test.txt', error_bad_lines=False, engine='python',sep='\s',header=None)
testing.head()

len(testing)

sentences = []
labels = []
i =0
while i < (len(testing)):
  sentence = []
  label = []

  while (testing[0][i] != ".") and (i < (len(testing))):
    sentence.append(testing[0][i])
    label.append(testing[1][i])
    i = i+1
  i = i+1
  sentences.append(sentence)
  labels.append(label)

print(sentences)
print('----',len(sentences),'----')
print(labels)
print('----',len(labels),'----')

"""calculating f1 scores"""

tp =0
fp =0
tn =0
fn =0
for i in range(len(sentences)):
  sentence = ''
  for j in range(len(sentences[i])):
    sentence += sentences[i][j]
    sentence += ' '
  docx = nlp_model(sentence)
  identified_as_street = []
  for entity in docx.ents:
      num = entity.start
      identified_as_street.append(num)
      if(entity.label_ == 'CHEMICAL'):
        print(entity, entity.label_, entity.start, labels[i][num])
  for k in range(len(labels[i])):
    if labels[i][k] == 'O' and k not in identified_as_street:
      tn+=1
    if labels[i][k] == 'O' and k in identified_as_street:
      fp +=1
    if labels[i][k] != 'O' and k not in identified_as_street:
      tn +=1
    if labels[i][k] != 'O' and k in identified_as_street:
      tp +=1
print('------------------------------------------------------')
precision = tp/(tp+fp)
recall = tp/(tp+fn)
accuracy = (tp+tn)/(tp+tn+fp+fn)
print('precision = ', precision)
print('recall = ', recall)
print('accuracy = ', accuracy)
print('F1 Score: ', (precision*recall)/(precision+recall))

"""making inferences"""

drug_pages = 0
page_traversed = 0
counter = 0
chemical_pages = 0
street_pages = 0
both_present = 0
either_present = 0
names_of_chemical = []
names_of_street = []
for text in all_data[:]:
    identified_as_chemical = 0
    identified_as_street = 0
    page_traversed +=1
    doc = nlp_model(text)
    result = [(ent,ent.label_) for ent in doc.ents]
    for k in range(len(result)):
      name = str(result[k][0])
      if name in chemical:
        identified_as_chemical=1
        names_of_chemical.append(name)
      if name in street:
        identified_as_street=1
        names_of_street.append(name)

    if len(result):
        check = 1
        drug_pages += 1
    if identified_as_chemical == 1:
        chemical_pages += 1
    if identified_as_street == 1:
        street_pages += 1
    if identified_as_street == 1 and identified_as_chemical == 1:
      both_present+=1
    if identified_as_street == 1 or identified_as_chemical == 1:
      either_present+=1

    else:
        check = 0

from collections import Counter

Counter_chem = Counter(names_of_chemical)
print(names_of_chemical)
most_common_chem = Counter_chem.most_common(5)
#print(most_common_chem)

Counter_street = Counter(names_of_street)
most_common_street = Counter_street.most_common(5)
#print(most_common_street)

most_freq5_street = []
most_freq5_chem = []

for i in range(5):
  most_freq5_street.append(most_common_street[i][0])
  most_freq5_chem.append(most_common_chem[i][0])

print(most_freq5_chem)
print(most_freq5_street)

print("Total pages viewed:", page_traversed)
print("Pages related to drugs:", either_present)
print("Pages related to chemicals:", chemical_pages)
print("Pages related to street:", street_pages)
print("Pages containing both chemical and street names: ", both_present)
print("Pages related chemical name or street name: ", either_present)
print("Average number of pages with street names: ", street_pages/page_traversed)
print("Average number of pages with chemical names: ", chemical_pages/page_traversed)
print('5 most frequent chemical names: ', most_freq5_chem)
print('5 most frequent street names: ', most_freq5_street)

print(names_of_street)

df = pd.DataFrame(names_of_chemical, columns = ['names_of_chemical'])
df['names_of_chemical'].value_counts()[:10].plot(kind='barh',)

# Commented out IPython magic to ensure Python compatibility.
#Importing Libraries
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from wordcloud import WordCloud
text = " ".join(cat for cat in names_of_street)
# Creating word_cloud with text as argument in .generate() method
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)
# Display the generated Word Cloud
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()