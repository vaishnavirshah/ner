# -*- coding: utf-8 -*-
"""NER_TestingOnCrawlData_Method1A

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uRqK0J-8JPsZBzcu8DsS5ODAxHnLsyYt

Imports
"""

pip install plac

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/RA/'

from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from tqdm import tqdm
from wordcloud import WordCloud, STOPWORDS
from spacy.util import minibatch, compounding
from spacy.training.example import Example
import matplotlib
import re
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

nlp = spacy.load('en_core_web_sm')

"""Data for testing"""

test_data = pd.read_csv(path+'DRUG_CROSSNER-main/ner_data/drugs/test_updated.txt', error_bad_lines=False, engine='python',sep='\s',header=None)
test_data.head()

len(test_data)

"""Training data - crawl_data"""

data = pd.read_csv(path+'MiniProject/crawl_data.csv', error_bad_lines=False, engine='python')
data.head()

"""chemical name csv"""

chemical_name = pd.read_csv(path+'MiniProject/drug_chem.csv', error_bad_lines=False, engine='python')
#chemical_name.head()
chemical = chemical_name['chemical_name'].unique().tolist()
chemical = [x.lower() for x in chemical]
print(chemical)

"""street name csv"""

street_name = pd.read_csv(path+'MiniProject/drug_street.csv', error_bad_lines=False, engine='python')
#street_name.head()
street = street_name['street_names'].unique().tolist()
more_names = street_name['more_names'].unique().tolist()
street = [x.lower() for x in street]

data = data.dropna()

data.info()

"""filtering the sentences, removing stopwords. Ref: mini project"""

stopwords = ['i', 'me', 'skip','section','-','site', 'no', 'javascript', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

def filtering(words):
    filtered_sentence = []
    words = word_tokenize(words)
    for w in words:
        if w in stopwords:
            continue
        else:
            filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: filtering(x)

data['modified_body'] = data['body'].apply(addwords)
data['modified_title'] = data['title'].apply(addwords)
data.drop('body', axis=1, inplace=True)
data.drop('description', axis=1, inplace=True)

data.head()

data.describe()

df = pd.DataFrame()
df = data['modified_body']

df.to_csv('TEXT.csv')

"""added potential street names from chemical.csv to street list"""

potential_street_name = chemical_name['potential_street_name'].unique().tolist()
potential_street_name = [str(x) for x in potential_street_name if (str(x) != 'nan') and (str(x) != '0')]
for i in potential_street_name:
  street.append(i)

more_names = [str(x) for x in more_names if (str(x) != 'nan') and (str(x) != '0')]
more_names = [x.lower() for x in more_names]

for i in more_names:
  street.append(i)

print(street)

"""combining the list of words to a string"""

def process_info(token_set): # basically combining it
  processed_info = []
  for token in token_set:
    token = ''.join(e.lower() for e in token if e.isalnum())
    processed_info.append(token)
  return ' '.join(processed_info)

"""preparing the training title and body"""

count = 0
Train_Data_Title =[]
for _, item in data.iterrows():
    ent_dict = {}
    if count<100000000:
        info = process_info(item['modified_title'])
        visited_items =[]
        entities = []

        for token in info.split():
          if token in chemical:
                for i in re.finditer(token, info):
                    if token  not in visited_items:
                        entity = (i.span()[0], i.span()[1], 'CHEMICAL')
                        visited_items.append(token)
                        entities.append(entity)
          if token in street:
            for i in re.finditer(token, info):
                if token  not in visited_items:
                    entity = (i.span()[0], i.span()[1], 'STREET')
                    visited_items.append(token)
                    entities.append(entity)

        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            Train_Data_Title.append(train_item)
            count+=1

train_data = []

count = 0
Train_Data_Body = []
all_data = []

for _, item in data.iterrows():
    ent_dict = {}
    if count<50000:
        category  = ''
        info = process_info(item['modified_body'])
        all_data.append(info)
        visited_items =[]
        entities = []
        for token in info.split():
            if token in street:
               for i in re.finditer(token, info):
                if token  not in visited_items:
                    entity = (i.span()[0], i.span()[1], 'STREET')
                    visited_items.append(token)
                    entities.append(entity)
                category = 'street'
            if token in chemical:
              for i in re.finditer(token, info):
                  if token  not in visited_items:
                      entity = (i.span()[0], i.span()[1], 'CHEMICAL')
                      visited_items.append(token)
                      entities.append(entity)
              category = 'chemical'
            if token in chemical and token in street:
              category = 'streetnchem'

        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            Train_Data_Body.append(train_item)
            count+=1
        else:
          category = 'non_drug'
        item = (info, category)
        train_data.append(item)

len(Train_Data_Title)

len(Train_Data_Body)

train_data

dataf = pd.DataFrame(train_data, columns=['text','category'])

dataf.to_csv('data.csv')

"""training the model"""

n_iter = 5
def train_ner_model(train_data):
    training_data = train_data
    nlp = spacy.blank("en")

    if "ner" not in nlp.pipe_names:
        nlp.add_pipe('ner')
        ner = nlp.get_pipe("ner")
    else:
        ner = nlp.get_pipe("ner")

    for _, annotations in training_data:
        for ent in annotations.get("entities"):
            ner.add_label(ent[2])

    nlp.begin_training()
    for itn in range(n_iter):
        random.shuffle(training_data)
        losses = {}

        batches = minibatch(training_data, size=compounding(4.0,32.0,1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            example = []
            for i in range(len(texts)):
                doc = nlp.make_doc(texts[i])
                example.append(Example.from_dict(doc, annotations[i]))
            nlp.update(example,drop=0.5,losses = losses)
        print("Epoch: ",itn,' ', losses)
    return nlp

nlp_model = train_ner_model(Train_Data_Title)

nlp_model.to_disk(path+'NER_TestingOnCrawlData/')

example_statement = "I think Drugs like acid, Cannabis, cocaine Ecstacy should be done with hash and pot at a party in Mumbai"

Train_Data_Body[0][0]

docx2 = nlp_model(Train_Data_Body[0][0])
for entity in docx2.ents:
  print(entity, entity.label_, entity.start, entity.end)

"""calculating the f1 scores"""

def calculate(test_data_body):
  tp =0
  fp =0
  tn =0
  fn =0
  for i in range(len(test_data_body)):
    identified_as_street = []
    sentence = test_data_body[i][0]
    docx = nlp_model(sentence)
    for entity in docx.ents:
        identified_as_street.append(entity.start)
    is_street = []
    for j in range(len(test_data_body[i][1]['entities'])):
      start = test_data_body[i][1]['entities'][j][0]
      k =0
      wordNum = 0
      while k != start:
        if(test_data_body[i][0][k] == ' '):
          wordNum+=1
        k+=1
      is_street.append(wordNum)
    for k in range(len(sentence.split())):
      if k not in is_street and k not in identified_as_street:
        tn+=1
      if k not in is_street and k in identified_as_street:
        fp +=1
      if k in is_street and k not in identified_as_street:
        fn +=1
      if k in is_street and k in identified_as_street:
        tp +=1
  print('------------------------------------------------------------------------------------------')
  precision = tp/(tp+fp)
  recall = tp/(tp+fn)
  accuracy = (tp+tn)/(tp+tn+fp+fn)
  print('precision = ', precision)
  print('recall = ', recall)
  print('accuracy = ', accuracy)
  print('F1 Score: ', (precision*recall)/(precision+recall))

calculate(Train_Data_Body[:20])

"""making inferences"""

drug_pages = 0
page_traversed = 0
counter = 0
chemical_pages = 0
street_pages = 0
both_present = 0
either_present = 0
names_of_chemical = []
names_of_street = []

false_pos = []

#while counter <= 15:

for text in all_data[:]:
    identified_as_chemical = 0
    identified_as_street = 0
    page_traversed +=1
    doc = nlp_model(text)
    result = [(ent,ent.label_) for ent in doc.ents]
    for k in range(len(result)):
      name = str(result[k][0])
      if name in chemical:
        identified_as_chemical=1
        names_of_chemical.append(name)
      if name in street:
        identified_as_street=1
        names_of_street.append(name)

    if len(result):
        check = 1
        drug_pages += 1
    if(len(result) > 0 and identified_as_street == 0 and identified_as_chemical == 0):
      for k in range(len(result)):
        name = str(result[k][0])
        false_pos.append(name)
    if identified_as_chemical == 1:
        chemical_pages += 1
    if identified_as_street == 1:
        street_pages += 1
    if identified_as_street == 1 and identified_as_chemical == 1:
      both_present+=1
    if identified_as_street == 1 or identified_as_chemical == 1:
      either_present+=1

    else:
        check = 0

false_pos

all_names = names_of_street
for i in names_of_chemical:
  all_names.append(i)

# Commented out IPython magic to ensure Python compatibility.
#Importing Libraries
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from wordcloud import WordCloud
text = " ".join(cat for cat in names_of_street)
# Creating word_cloud with text as argument in .generate() method
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)
# Display the generated Word Cloud
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""calculating frequent words"""

from collections import Counter

Counter_chem = Counter(names_of_chemical)
most_common_chem = Counter_chem.most_common(5)

Counter_street = Counter(names_of_street)
most_common_street = Counter_street.most_common(5)

most_freq5_street = []
most_freq5_chem = []

for i in range(5):
  most_freq5_street.append(most_common_street[i][0])
  most_freq5_chem.append(most_common_chem[i][0])

print(most_freq5_chem)
print(most_freq5_street)

Counter_chem

print("Total pages viewed:", page_traversed)
print("Pages related to drugs:", either_present)
print("Pages related to chemicals:", chemical_pages)
print("Pages related to street:", street_pages)
print("Pages containing both chemical and street names: ", both_present)
print("Pages related chemical name or street name: ", either_present)
print("Average number of pages with street names: ", street_pages/page_traversed)
print("Average number of pages with chemical names: ", chemical_pages/page_traversed)
print('5 most frequent chemical names: ', most_freq5_chem)
print('5 most frequent street names: ', most_freq5_street)

df = pd.DataFrame(names_of_chemical, columns = ['names_of_chemical'])
ax = df['names_of_chemical'].value_counts()[:10].plot(kind='barh')
# ax.set_xlabel("Number of webpages with mentioned chemical names of drugs")
# ax.set_ylabel("Chemical names of drugs")

df = pd.DataFrame(names_of_street, columns = ['names_of_street'])
ax = df['names_of_street'].value_counts()[:10].plot(kind='barh')
# ax.set_xlabel("Number of webpages with mentioned street names of drugs")
# ax.set_ylabel("Street names of drugs")