# -*- coding: utf-8 -*-
"""Copy of NER-2ndapp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ooqO0OU-w1lbV4haZ0ouW3epOFeaINAn
"""

pip install plac

!python -m spacy download en_core_web_lg

from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from tqdm import tqdm
from wordcloud import WordCloud, STOPWORDS
from spacy.util import minibatch, compounding
from spacy.training.example import Example
import matplotlib
import re
import json
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

nlp1 = spacy.load("en_core_web_lg")

path = '/content/drive/MyDrive/RA/MiniProject/'

chemical_name = pd.read_csv(path+'drug_chem.csv', error_bad_lines=False, engine='python')
chemical_name.head()

street_name = pd.read_csv(path+'drug_street.csv', error_bad_lines=False, engine='python')
street_name.head()

street = street_name['street_names'].unique().tolist()
more_names = street_name['more_names'].unique().tolist()
street = [x.lower() for x in street]
street.append('street')

#street

chemical = chemical_name['chemical_name'].unique().tolist()
chemical = [x.lower() for x in chemical]
print(chemical)

pip install sense2vec

nlp_model = spacy.load(path+'ner_400its')

nlp_model.to_disk(path+'ner_400its')

example_statement = 'currency  euro  pound sterling  us dollar vendor account my account register login wish list  0  shopping cart checkout 0 item    000 your shopping cart empty  categories digital electronic products  8  phone  3  computer  5  hacking service  3  show all digital pills adderall  2  ritalin  2  xanax  4  oxycodone  3  tramadol  2  suboxone  1  morphine  1  methadone  2  diazepam  4  valium  4  alprazolam  2  captagon  1  targin  1  hydrocodone  1  yaba  2  testosteron enanthate  2  show all pills stimulants cocaine  7  mdma  6  ecstasy  xtc  9  meth  6  fentanyl  2  ketamine  4  ghb  1  potassium cyanide  2  show all stimulants cannabis weed  10  hash  10  kush  10  vape  3  capsules  1  show all cannabis psychedelic lsd  3  dmt  2  show all psychedelic money transfers western union transfer  4  paypal transfer  4  moneygram transfers  4  show all money transfers credit card cccvv  5  show all credit card gift cards amazon gift cards  0  ebay gift cards  0  itunes gift cards  0  steam gift cards  0  google play store gift cards  0  show all gift cards counterfeits money weapons  guns pistols  3  rifles  1  ammo  bullet  3  show all weapons  guns links onion  0  market  0  marketplace  0  dir  onion dir  0  drugs  0  bitcoin  0  escrow service  0  search engine  0  darknet market  0  deepweb market  0  show all links about venus marketplace welcome venus marketplace  venus marketplace established 2018  it among best marketplaces darknet  it ranks high search engines  you browse products place orders  the website keeps records encrypted hashes  it gives importance seller buyer security  escrow service available  refund seller nt ship products  products calculated dollars  payments made bitcoin  shipping countries  cargo content confidential secure  website vendors products 100  verified  25  discount express cargo purchases  250  coupon code  vm250 support questions  venusmarketcontact  protonmailcom money transfers western union transfer digital  11  pills  31  stimulants  37  cannabis  14  psychedelic  5  money transfers  12  western union transfer  4  paypal transfer  4  moneygram transfers  4  credit card  5  gift cards  0  counterfeits money  0  weapons  guns  7  links  0  western union transfer product compare  0  sort by  default name  a z  name  z a  price  low  high  price  high  low  model  a z  model  z a  show  15 25 50 75 100 western union transfer 1000  we specialized 2 years  experience field  we also offer full refund  via   10000 add cart western union transfer 3000  we specialized 2 years  experience field  we also offer full refund  via   20000 add cart western union transfer 5000  we specialized 2 years  experience field  we also offer full refund  via   30000 add cart western union transfer 10000  we specialized 2 years  experience field  we also offer full refund  via   50000 add cart showing 1 4 4  1 pages  information about us delivery information privacy policy terms  conditions customer service contact us returns site map vendor vendor account vendor register verify my vendor account my account my account order history wish list venus market  2018 2021'

docx2 = nlp_model(example_statement)
for entity in docx2.ents:
    print(entity, entity.label_, entity.start_char, entity.end_char, entity.start, entity.end)

if 'fentanyl' in street:
  print(True)

data = pd.read_csv(path+'crawl_data.csv', error_bad_lines=False, engine='python')
data.head()

data = data.dropna()

data.info()

stopwords = ['i', 'me', 'skip','section','-','site', 'no', 'javascript', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

def apwords(words):
    filtered_sentence = []
    words = word_tokenize(words)
    for w in words:
        if w in stopwords:
            continue
        else:
            filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: apwords(x)

data['modified_body'] = data['body'].apply(addwords)
data['modified_title'] = data['title'].apply(addwords)
data.drop('body', axis=1, inplace=True)
data.drop('description', axis=1, inplace=True)

def process_info(token_set): # basically combining it
  processed_info = []
  for token in token_set:
    token = ''.join(e.lower() for e in token if e.isalnum())
    processed_info.append(token)
  return ' '.join(processed_info)

with open(path+'drugs_patterns.jsonl', 'r') as json_file:
    json_list = list(json_file)
drugs = []
for i in range(len(json_list)):
  test=json.loads(json_list[i])
  drugs.append(test['pattern'][0]['lower'])

count = 0
Train_Data_Body = []
for _, item in data.iterrows():
    ent_dict = {}
    if count<1000:
        info = process_info(item['modified_body'])

        visited_items =[]
        entities = []

        for token in info.split():
            if token in street:
              #print(token)
              for i in re.finditer(token, info):
                  if token  not in visited_items:
                      entity = (i.span()[0], i.span()[1], 'Street', token)
                      #print(entity)
                      visited_items.append(token)
                      entities.append(entity)
              if token in chemical:
              #print(token)
                for i in re.finditer(token, info):
                    if token  not in visited_items:
                        entity = (i.span()[0], i.span()[1], 'Chemical', token)
                        print(entity)
                        visited_items.append(token)
                        entities.append(entity)
        if len(entities) > 0:
            ent_dict['entities'] = entities
            train_item = (info, ent_dict)
            #print(train_item)
            Train_Data_Body.append(train_item)
            count+=1

len(Train_Data_Body)

if '1-morphine 1-methadone' in chemical_name:
  print(True)

test_data = Train_Data_Body[:100]

def check_fn(drug_start,identified_start, identified_end):
  counter = 0
  #print((drug_start), identified_start, identified_end)
  for i in range(len(drug_start)):
    marked = 1
    for j in range(len(identified_start)):
      #print(drug_start[i], identified_start[j], identified_end[j])
      if drug_start[i] >= identified_start[j] and  drug_start[i] <= identified_end[j]:
        marked = 0
    counter += marked
  #print(counter)
  return counter

from nltk.corpus.reader.wordnet import WordNetICCorpusReader
tp =0
fp =0
tn =0
fn =0

words = []
for i in range(len(test_data)):
  sentence = test_data[i][0]
  labels = test_data[i][1]
  drug_start = []
  #print(len(sentence.split()))
  for l in range(len(test_data[i][1]['entities'])):
    drug_start.append(test_data[i][1]['entities'][l][0])
  #print(drug_start)
  docx = nlp_model(sentence)

  identified_start = []
  identified_end = []
  for entity in docx.ents:
      #print(entity,entity.start_char,entity.end_char, entity.label_)
      identified_start.append(entity.start_char)
      identified_end.append(entity.end_char)
      for word in (str(entity)).split():
        if word in street or word in chemical:
          tp+=1
        words.append(word)
  fn += check_fn(drug_start, identified_start, identified_end)
  start_of_word = 0
  for k in range(len(sentence)):
    if start_of_word == 0:
      if k not in identified_start and k not in drug_start:
        tn += 1
    if(sentence[k] == ' '):
      start_of_word = 0
    else:
      start_of_word = 1

words = set(words)
for word in words:
  if word not in street or word not in chemical:
    fp+=1


print('TP : ',tp)
print('FP : ',fp)
#print('TN : ',tn)
print('FN : ',fn)
precision = tp/(tp+fp)
recall = tp/(tp+fn)
#accuracy = (tp+tn)/(tp+tn+fp+fn)
print('precision = ', precision)
print('recall = ', recall)
#print('accuracy = ', accuracy)
print('F1 Score: ', (precision*recall)/(precision+recall))

